Test Case 1:
Scenario:
A single client, with a pseudorandom workload. No failures are injected. Test cases ensure the following (among other checks):
1. Olympus must create initial configuration - keys, create, setup, and start processes.
2. Client sends the request to head and waits for a response
3. Head assigns slot, signs order statement, result statement and sends the shuttle along the chain.
4. Replicas must support dictionary operations - put, get, slice, append.
5. Replica checks for validity of order proof

Client Request: pseudorandom(233,20)
Failure Scenario: No failures injected
Commands:
./run_main_1.sh
./run_client.sh
Log File:
Logs for Client: client.log
Replica, Configuration and Olympus related logs: main.log
Programmatic Check:
run_state_consistency_test.sh
To check if the local state of the client is same as the response received.
Outcome: Replica receives a response (a valid result or a an empty result) based on the sequence of operations from the pseudorandom request.


Test Case 2:
Scenario:
Multi-Client Workload. Inject a client Retransmission scenario. Check for the following scenario:

1. Multiple clients, with a sequence of workload as specified in the config file.
2. Check if request sequence is generated as specified in config file.
3. The client must simulate the retransmission described in the paper by sleeping till it times out.
4. Check if client correctly times out and sends a request to all replicas if timely response not received.
5. Head should handle the retransmissions as described in the paper (this showcased across multiple test cases).
6. All non-head replicas must send cached result if present.

Client Request:
Client 1 Workload = pseudorandom(233,5)
Client 2 Workload = put('movie','star'); append('movie',' wars'); get('movie')
Client 3 Workload = put('jedi,'luke skywalker); slice('jedi','0:4'); get('jedi')

Failure Scenario: Inject Sleep() to tail
Commands:
./run_main_2.sh
./run_client.sh
Log File:
Logs for Client: client.log
Replica, Configuration and Olympus related logs: main.log

Outcome: Client times out after not receiving the request. Client retransmits the request. Retransmission is successfully processed. The client receives the response.


Test Case 3:
Scenario:
Single Client Workload. The client sends a single retransmission request to a non-head replica. Replica sends a forward request to head.
A change_operation() failure is injected with the trigger: forwarded_request(0,0). The state consistency check fails.

Client Request:
Client 1 - put('movie','star'); append('non_existent_key',' wars'); get('movie')

Failure Scenario: change_operation()
Commands:
./run_main_3.sh
./run_client.sh
Log File:
Logs for Client: client.log
Replica, Configuration and Olympus related logs: main.log
Programmatic Check:
run_state_consistency_test.sh
To check if the local state of the client is same as the response received.
Outcome: The state consistency check fails, as expected.


Test Case 4:
Scenario:
Single client workload. Head's result statement from the result proof is omitted for shuttle(0,m) where 0 represents the first client and m is message number.

Client Request: put('movie','star'); append('movie',' wars'); get('movie')
Failure Scenario: drop_result_stmt()
Commands:
./run_main_4.sh
./run_client.sh
Log File:
Logs for Client: client.log
Replica, Configuration and Olympus related logs: main.log

Outcome: Result proof verification fails at replica as expected.


Test Case 5:
Scenario:
Single client workload. Inject a failure for the trigger: shuttle(0,m), where 0 represents the 1st client and m is the message number. Check if order proof verification fails.
Client Request: put('movie','star'); append('movie',' wars'); get('movie')
Failure Scenario: change_operation()
Commands:
./run_main_5.sh
./run_client.sh
Log File:
Logs for Client: client.log
Replica, Configuration and Olympus related logs: main.log
Outcome: Reconfigure request will be raised by the replica.


Test Case 6:
Scenario:
Stress Test - Carrying out stress tests with:
(1000 requests, 10 clients, 3 replicas)
(1000 requests, 10 clients, 7 replicas)
(2000 requests, 1 client, 11 replicas)

Client Request:
Failure Scenario:
Commands:
Log File:
Logs for Client: client.log
Replica, Configuration and Olympus related logs: main.log
Programmatic Check:
Outcome: Success: All three cases were handled successfully without crashes.

Phase 3 Test Cases:
Test Case 1:
Scenario:
The Head replica injects a sleep failure and triggers a reconfiguration
This test case demonstrates the following features:
Client:
    Client sends configuration request for every ten operation requests or before retransmitting a request.
Olympus:
    Olympus sends wedge requests
    Olympus validates wedge messages sent by replicas
    Olympus selects the quorum and checks for its consistency
    Olympus computes the longest history and asks other replicas to catchup
    Olympus verifies the running states of all the replicas in the quorum by sending 'get_running_state' messages
    If consistent state is found, it is used the initial running state for the new configuration
    else Olympus finds a new quorum

Client Request: workload[0] = pseudorandom(233,20)
Failure Scenario:
failures[0,0] = client_request(0,9), sleep(5000)
Commands:
./run_main_phase_3_2.sh
> cd src
> python state_test.py
Log File:
Logs for Client: client.log
Logs for Olympus: olympus.log
Logs for Replica: replica.log
Programmatic Check:
run_state_consistency_test.sh
To check if the local state of the client is same as the response received.
Outcome:
System reconfigures successfully and responds to all the client requests. The states are consistent


Test Case 2:
Scenario:
Replica 2 upon receiving the first checkpoint message in a checkpoint shuttle, will put an invalid signature on
its order statement. Order Proof verification fails in the next replica and Reconfiguration request is raised. Olympus then sends a
wedge statement to all the replicas and forms a quorum, we observe that it usually takes two reconfigurations to find the
consistent state. The entire process covers the following cases:
1. Head and Non Head timeouts (reconfiguration-request if timeout ) CASE 1 and CASE 2
2. Detect provable misbehavior and send reconfiguration-request - Order Proof Validation fails
3. head: periodically initiate checkpoint, send checkpoint shuttle
4. non-head: add signed checkpoint proof, send updated checkpoint shuttle
5. handle completed checkpoint shuttle: validate completed checkpoint proof
6. delete history prefix, forward completed checkpoint proof
7. handle catch-up message, execute operations, send caught-up message

Client Request: Pseudorandom workload, 1 client, 20 requests.
Failure Scenario:
failures[0,1] = checkpoint(0),invalid_order_sig(); catch_up(0),drop()
OR/AND
failures[0,1] = complete_checkpoint(0),drop_checkpt_stmts()

Commands: run_main_phase_3_2.sh
Log File:
Logs for Client: client.log
Logs for Olympus: olympus.log
Logs for Replica: replica.log

Programmatic Check:
run_state_consistency_test.sh
To check if the local state of the client is same as the response received.
Outcome:
System reconfigures successfully and responds to all the client requests. The states are consistent

Test Case 3:

Scenario:
Runs the perform900 test case
Replicas: 7
Client Request: Pseudorandom workload, 3 client, 300 requests, total 900 requests

Commands: run_main_phase_3_2.sh
Log File:
Logs for Client: client.log
Logs for Olympus: olympus.log
Logs for Replica: replica.log

Programmatic Check:
run_state_consistency_test.sh
To check if the local state of the client is same as the response received.
Outcome:
System successfully handles all the 900 requests.

Test Case 4:
Scenario:
Demonstrate each replica running on separate hosts
Log File:
Logs for Client: client.log
Logs for Olympus: olympus.log
Logs for Replica: replica.log
Outcome:
System successfully runs replicas on separate hosts

PERFORMANCE EVALUATION

Raft: 13.676953554153442 seconds
Byzantine Chain Replication:
    Single Host: 66.50380516052246 seconds
    Multiple Host: 70.55077481269836 seconds.
Note:
Multiple Host: Client and replica on host1 and each replica on its own host
Single Host: Client and replica on host1 and all replicas on host2
